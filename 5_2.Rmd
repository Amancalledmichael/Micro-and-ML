---
title: "R Notebook"
output: html_notebook
---

# Problem Set 5 

# Problem 5.2

```{r}
library(tidyverse)
library(stargazer)
library(rpart)
library(rpart.plot)

#read the data
health_raw <- read_csv("health.csv")

#present first 6 rows of health data
health_raw %>% head()

dummies <- c("suppins", "female","white", "hisp",  "marry", "northe" , 
              "mwest", "south", "msa","phylim","actlim","injury","priolist", "hvgg")


health <- health_raw
health[, dummies] <- lapply(health[,dummies], factor)
```

__a) Generate an enlarged regressor set, containing the original varibles in $x_{i}$ and
• all interaction terms and
• the squared terms of the continuous variables of $x_{i}$.__


__Explain, why adding squared terms of the dummy variables does not make sense__

* It does not make sense because 1 x 1 = 1 and 0 x 0 = 0, such that squared dummies are the same as the dummies 

```{r}
# function to add interaction term between all variables of a df except the dep_var
add_inter_terms <- function(df, dep_var, dummies = NULL){
  vars <- colnames(df)[! colnames(df) %in% dep_var]
  for (i in vars) {
    for (j in vars) {
      if (i < j ) {
        df[paste0(i, "X", j)] <- df[i] * df[j]
        if (i %in% dummies && j %in% dummies) {
          df[[paste0(i, "X", j)]] <- factor(df[[paste0(i, "X", j)]])
        } 
      }
    }  
  }
  df
}

# function to add squared terms to df for specified vars
add_squared_terms <- function(df, vars) {
  for (i in vars) {
    df[paste0(i, "X", i)] <- df[i] * df[i]
  }
  df
}

# variables to be squared
continuous_vars <- c("income", "educyr", "age", "famsze", "totchr")

# extend dataframe
health_extend <- add_inter_terms(health_raw, c("ltotexp", "suppins"), dummies = dummies)
health_extend <- add_squared_terms(health_extend, continuous_vars)
health_extend[,dummies] <- lapply(health_extend[,dummies], factor)
```


__b) Denote the vector of extended explanatory varibles by $x_{e,i}$. Standardize the regressors in $x_{i}$ and $x_{e,i}$, respectively.__

```{r}
health[, continuous_vars] <- scale(health[, continuous_vars]) 

health %>% head()


scale_all_but <- function(df, but) {
  for (var in colnames(df)[! colnames(df) %in% but]){
    if (class(df[[var]]) == "numeric") {
      df[var] <- scale(df[var])
    }

  }
  df
}

health_extend <- scale_all_but(health_extend, c("ltotexp", "suppins"))
health <- scale_all_but(health, c("ltotexp", "suppins"))
health_extend %>% head()
```

__c) As naive approach to estimate the ATE, estimate model 1 by OLS, using $g(x) = x_{i}′β$. Also obtain the 95% confidence bounds.__

```{r}
OLS <- lm(ltotexp ~ 0 + ., data = health)
stargazer(OLS, type = "text", keep = "suppins1", ci = TRUE,
          omit.stat = "all")
```

__d) Comment on potential problems with the naive approach.__

It is very unlikely that the Unconfoundedness assumption holds, which in turn does not allow to indentify the ATE. Further, the functional form might be misspecified because interaction terms and non-linear effects are not included in the regression.


### Lassoing

__e) Estimate f1 and f2 using the LASSO framework and the extended regressors $x_{e,i}$. Use 5-fold cross-validation to tune the penalty parameter.__

```{r}
# install.packages("glmnet")
library(glmnet)
set.seed(1887)
# dataframes as matrices (throw out NAs)
x <- data.matrix((health_extend[complete.cases(health_extend), ! names(health_extend) %in% c("ltotexp", "suppins")]))
ltotexp <- data.matrix(health_extend[complete.cases(health_extend), "ltotexp"])
suppins <- data.matrix(health_extend[complete.cases(health_extend), "suppins"]) - 1

# find best lambdas
lambdas <- 10^seq(2, -3, by = -.1)

cv_f1 <- cv.glmnet(x = x, y = ltotexp, alpha  = 1, nfolds = 5, standardize = F)
best_lambda_f1 <- cv_f1$lambda.min
best_lambda_f1
plot(cv_f1)

cv_f2 <- cv.glmnet(x = x, y = suppins, alpha  = 1, nfolds = 5, family = "binomial")
best_lambda_f2 <- cv_f2$lambda.min
best_lambda_f2
plot(cv_f2)

# Estimate LASSOs
lasso_f1 <- glmnet(x = x, y = ltotexp, alpha = 1, lambda = best_lambda_f1)
coef(lasso_f1)

lasso_f2 <- glmnet(x = x, y = suppins, alpha = 1, lambda = best_lambda_f2, family = "binomial")
coef(lasso_f2)
```



__f) Based on the residuals from the LASSO estiamtions, estimate α and its 95% confidence bounds.__

```{r}
# robust standard errors
# install.packages("sandwich")
library(sandwich)

# predicted values from LASSO estimates
predicted_ltotexp <- predict(lasso_f1, newx = x)
residuals_ltotexp <- ltotexp - predicted_ltotexp

predicted_suppins <- predict(lasso_f2, newx = x, type = "class")
residuals_suppins <- suppins - as.double(predicted_suppins)

# OLS of residuals from ltotexp on residuals from suppins
alpha_lasso <- lm(residuals_ltotexp ~ 0 + residuals_suppins)


# robust variance-covariance matrix

calc_CI <- function(coef, Variance, n) {
		up_bound <- coef + 1.96 * sqrt(Variance/n)
		low_bound <- coef - 1.96 * sqrt(Variance/n)
		tibble(lower_bound = low_bound,
		        upper_bound = up_bound)
}

V <- function(residuals_D, residuals_OLS, n) {
  (1 / ((n/(n-1)) * mean(residuals_D ^ 2))) * (n/(n-1)) * mean(residuals_OLS ^ 2 * residuals_D ^ 2) * (1 / ((n/(n-1)) * mean(residuals_D ^ 2)))
}

variance_lasso <- V(residuals_suppins, alpha_lasso$residuals, nrow(health_extend))

CI_lasso <- calc_CI(alpha_lasso$coefficients[["residuals_suppins"]], variance_lasso, nrow(health_extend))

alpha_lasso$coefficients[["residuals_suppins"]]
CI_lasso

```


__g) Discuss similiarities and conceptual differences between choosing the optimal penalty parameter
via cross-validation or information criteria.__

Both aim at finding the optimal penalty parameter, to minimize the test error.

Cross-Validation:

* directly estimates test error

* depends on splits

Information Criteria:

* indirectly estimates test error

* uses adjustments to the training error to account for bias due to overfitting

### Regression Tree

__h) Discuss, why using the extended variables set $x_{e,i}$ instead of using $x_{i}$ is superfluous(= redundant) according the fundamental concept of regression trees.__

Regression trees include interactions and polynomial terms by construction. 


__i) When growing a decision tree, you have to decide about the desired level of complexity.
Discuss, how the following hyper-parameters affect model complexity:
• maximum depth of the tree
• minimum number of observations in a leaf node
• minimum improvement of performance induced by a split__

They all reduce the complexity of the tree:

* maximum depth: limits the tree size, seems somwhat arbitrary but can be optimized by using e.g. cross validation. 

* minimum nr of obs in leaf node: Minimum number of observations that must be in each leaf, avoids overfitting.

* minimum improvement of performance induced by a split: compares performance of tree before a split and after a split and then only uses the split if the performance increases by some specified amount. Makes sense, but sometimes splits that dont lead to strong improvements lead to splits with strong improvements after.

__j) Estimate f1 and f2 by means of a regression tree. Use a maximum depth of 5 and make sure that in each leaf are at least 20 observations. Set the minimum improvement necessary for a further split to zero.__

```{r}
tree_f1 <- rpart(ltotexp ~ . -suppins , data = health,
            minsplit = 0, minbucket = 20, maxdepth = 5, cp = 0.00, xval = 0)


tree_f2 <- rpart(suppins ~ . -ltotexp, data = health,
            minsplit = 0, minbucket = 20, maxdepth = 5, cp = 0.00, method = "class")


# plot f1
rpart.plot(tree_f1,
           tweak = 1,# we can tweak the size of the tree
           type = 5, # we can change the display of the decision nodes
           extra = 1) # we can change the display information of the terminal nodes


rpart.plot(tree_f2,
           tweak = 1,# we can tweak the size of the tree
           type = 5, # we can change the display of the decision nodes
           extra = 1) # we can change the display information of the terminal nodes

```


__k) Briefly explain, why setting the minimum improvement for a split to zero still does not allow for infinite model complexity in this case.__

* Because we set the max depth to 5 and the minimum leaf size to 20 such that we cannot split w/o limit.

__l) Based on the residuals from the trees, estimate α and its 95% confidence bounds.__

```{r}
residuals_f1_tree <- residuals(tree_f1)
residuals_f2_tree <-  as.double(health$suppins) - as.double(predict(tree_f2, select(health, -suppins), type = "class"))

alpha_tree <- lm(residuals_f1_tree ~ 0 + residuals_f2_tree)

variance_tree <- V(residuals_f1_tree, alpha_tree$residuals, nrow(health))
CI_tree <- calc_CI(alpha_tree$coefficients[[1]], variance_tree, nrow(health))

alpha_tree$coefficients
CI_tree
```


### Conclusions

__m) Compare the estimates of α that you obtain via the partial linear model estimations (Lasso/tree) with the naive estimate from c).__

__n) Check, whether the confidence bounds of the three estimates of α are overlapping. Discuss your results. What is the value-added by estimating the partial linear model via LASSO/tree?__

__o) Discuss disadvantages of the partial linear model approach with respect to the treatment effect compared to e.g. propensity score weighting.__
